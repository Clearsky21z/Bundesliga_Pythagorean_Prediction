---
title: "Bundesliga Pythagorean Prediction"
subtitle: "Estimating league-specific coefficients (2010/11–2023/24) and prospective PiT accuracy in 2024/25"
author:
  - John Zhang
thanks: "Code and data: https://github.com/Clearsky21z/Bundesliga_Pythagorean_Prediction"
date: today
date-format: long
abstract: >
  We study a four-parameter Pythagorean points model for the German Bundesliga.
  Match-level CSVs are cleaned and standardized, season tables are constructed,
  and end-of-season (EoS) team totals are pooled across 2010/11–2023/24. We
  estimate league-specific coefficients (a, b, c, d) by minimizing mean absolute
  error (MAE) using a multi-start Nelder–Mead procedure. Generalization is evaluated
  with leave-one-season-out (LOSO) validation, summarizing accuracy by the
  median MAE and Pearson correlation between predicted and realized EoS points.
  Using the fitted coefficients, we generate “points-in-table” (PiT) forecasts
  for 2024/25 at 9, 18, and 27 rounds by combining realized points to date with
  Pythagorean-based projections for remaining fixtures. We report accuracy at
  each checkpoint, visualize predicted vs. actual points, and highlight the
  largest over- and under-performers.
format:
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
execute:
  echo: false
  warning: false
  message: false
  freeze: auto
bibliography: reference.bib
# Force figures/tables to appear exactly where placed
fig-pos: H
tbl-pos: H
header-includes: |
  \usepackage{float}
  \floatplacement{figure}{H}
  \floatplacement{table}{H}
---

\newpage

```{r setup}
library(readr)
library(dplyr)
library(ggplot2)
library(glue)
library(here)
library(knitr)

options(knitr.kable.NA = "")

OUT <- here("output")

coefs       <- read_csv(file.path(OUT, "bundesliga_coefs_pooled.csv"), show_col_types = FALSE)
in_sample   <- read_csv(file.path(OUT, "bundesliga_in_sample_by_season.csv"), show_col_types = FALSE)
loso        <- read_csv(file.path(OUT, "bundesliga_loso_cv.csv"), show_col_types = FALSE)
pool_pred   <- read_csv(file.path(OUT, "bundesliga_pool_with_pred.csv"), show_col_types = FALSE)
pit_summary <- read_csv(file.path(OUT, "BL_2024_25_PiT_summary.csv"), show_col_types = FALSE)
diff27      <- read_csv(file.path(OUT, "BL_2024_25_team_diffs_round27.csv"), show_col_types = FALSE)

coef_line <- glue("a={round(coefs$a[1],4)}, b={round(coefs$b[1],4)}, c={round(coefs$c[1],4)}, d={round(coefs$d[1],4)}")

ins_mae <- mean(abs(pool_pred$PTS - pool_pred$pred))
ins_r   <- suppressWarnings(cor(pool_pred$PTS, pool_pred$pred))
loso_med_mae <- median(loso$MAE, na.rm = TRUE)
loso_med_r   <- median(loso$r,   na.rm = TRUE)

fmt_num <- function(x, k=2) format(round(x, k), nsmall=k, big.mark=",")

```

# Introduction
A simple, durable idea in sports analytics is that scoring and conceding contain most of the information needed to forecast a team’s points total. In football this is commonly operationalised with a “Pythagorean” relationship between goals for (GF), goals against (GA), and expected points. Following the presentation in Chapter 5 of the Soccer Analytics textbook and Professor Clive Beggs’ work applying Pythagorean points to league football (@beggs), we tailor a four-parameter variant to the German Bundesliga and evaluate how well it predicts the final table during the 2024/25 season.

This paper makes three contributions. First, we construct a reproducible pipeline from raw match CSVs to end-of-season (EoS) team tables for the modern Bundesliga era (2010/11–2023/24). Second, we estimate **league-specific** coefficients ((a,b,c,d)) by minimizing mean absolute error on a pooled team–season panel, and we quantify transportability with leave-one-season-out (LOSO) validation. Third, we assess **prospective** “points-in-table” (PiT) forecasts for 2024/25 after 9, 18, and 27 rounds, combining realized points to date with Pythagorean projections for the remaining fixtures.

Methodologically we follow the four-parameter form popularized in the football literature: the expected points for team $i$ are $\widehat{\text{PTS}}_{i}=a\cdot\text{frac}_{i}\cdot\text{PLD}_{i}$ where $\text{frac}_{i}=\dfrac{GF_{i}^{\,b}}{GF_{i}^{\,c}+GA_{i}^{\,d}}$.

The paper proceeds as follows. Section @sec-methods formalizes the model and the
reproducible pipeline used to clean raw CSVs, construct season tables, estimate
coefficients, and perform validation. Section @sec-results reports fitted
coefficients, in-sample diagnostics (@tbl-in-sample), LOSO validation
(Table @tbl-loso), and prospective PiT performance for 2024/25
(Table @tbl-pit-summary; Figures @fig-r09–@fig-r27). Section @sec-discussion
interprets the findings, limitations, and extensions. All artifacts are read from
the `output/` directory.

# Methodology {#sec-methods}

## Model

For team–season observation $i$, with goals for $GF_i$, goals against $GA_i$,
and matches played $PLD_i$, we use
$$
\text{frac}_i = \frac{GF_i^{b}}{GF_i^{c} + GA_i^{d}}, \qquad
\widehat{PTS}_i = a \cdot \text{frac}_i \cdot PLD_i.
$$

Here $a>0$ scales the expected fraction to the 3-points-per-win system, and
$b,c,d>0$ control curvature and the relative impact of scoring vs conceding.

**Estimation.** Let $y_i$ be realized EoS points and $\hat{y}_i(a,b,c,d)$ the model prediction.
We choose $(a,b,c,d)$ to minimize mean absolute error (MAE) across the pooled
team–season set (2010/11–2023/24) using **Nelder–Mead** with multiple starts.

**Validation.** We perform **leave-one-season-out (LOSO)** validation: for each season $s$,
refit on all other seasons and evaluate on $s$. We summarize performance by
median MAE and median Pearson correlation $r$ across held-out seasons.

## Data {#sec-data}

### Data Overview {#sec-overview}

This study uses match-level Bundesliga CSV files covering the modern era (2010/11–2024/25). Each file contains all fixtures for a season with home/away teams, goals scored, and the full-time result. We transform these match records into season-level team summaries (end-of-season, EoS) and also build a stacked panel of team–season observations for 2010/11–2023/24 to estimate league-specific Pythagorean coefficients. The 2024/25 season is reserved for *prospective* evaluation using points-in-table (PiT) forecasts after 9, 18, and 27 rounds.

### Variables and Measurements

\begin{enumerate}
\item \textbf{Season}: File-level season tag (e.g., \texttt{D1\_2010\_2011.csv}) used as the season identifier.
\item \textbf{Team}: Club name in that season (character string, normalized in cleaning).
\item \textbf{PLD}: Matches played by the team in the league season (integer; Bundesliga: 34).
\item \textbf{GF}: Goals For — total goals scored by the team across league matches (integer).
\item \textbf{GA}: Goals Against — total goals conceded by the team across league matches (integer).
\item \textbf{PTS}: League points under the 3–1–0 system (3 for a win, 1 for a draw, 0 for a loss), computed from match results (integer).
\end{enumerate}

These variables are sufficient to compute the Pythagorean fraction and the implied points expectation without any external ratings or betting information.

### Dataset Selection

We estimate coefficients on the pooled panel of team–seasons from **2010/11–2023/24** only. Those years provide a consistent competitive format (18 teams, 34 rounds) and align with the time span used in our scripts. All seasons with clean, complete data in that window are included. The **2024/25** season is held out entirely for prospective testing: we form PiT predictions after 9, 18, and 27 rounds and compare them with the final 2024/25 table.

### Data Processing Tools

All steps are scripted in **R**:

* `script/01-data_cleaning.R`: drop empty columns, normalize team names, parse dates/times, and recompute `FTR` from goals.
* `script/02-data_aggregating.R`: build per-season EoS tables and a pooled stack.
* `script/03-fitting_coefficients.R`: filter to 2010/11–2023/24, fit ((a,b,c,d)) by MAE (multi-start Nelder–Mead), compute in-sample and LOSO metrics; writes CSVs to `output/`.
* `script/04-pit_testing_2024_25.R`: generate PiT evaluations for 2024/25 (Rounds 9/18/27) and save plots/tables to `output/`.

Core packages: @dplyr, @readr, @ggplot2, @here, @fs, @stringr, @knitr.


### Example: Bundesliga 2010/11 End-of-season Table

@tbl-eos-2010 shows the final Bundesliga standings for 2010/11 season as reconstructed from the cleaned match file.

```{r}
#| label: tbl-eos-2010
#| tbl-cap: "Bundesliga 2010/11 End-of-season Table"

library(readr)
library(dplyr)
library(knitr)
library(here)

pool_pred <- read_csv(here("output", "bundesliga_pool_with_pred.csv"),
                      show_col_types = FALSE)

eos_2010 <- pool_pred %>%
  filter(grepl("^D1_2010_2011", Season)) %>%
  mutate(GD = GF - GA) %>%
  select(Team, PLD, GF, GA, GD, PTS) %>%   # <- Season removed here
  arrange(desc(PTS), desc(GD), desc(GF))

kable(
  eos_2010,
  format = "latex",
  booktabs = TRUE,
  linesep = ""       
)
```

# Results {#sec-results}

## Fitted coefficients (2010/11–2023/24)

The pooled Bundesliga coefficients are **`r coef_line`**
(see `output/bundesliga_coefs_pooled.csv`).

### In-sample diagnostics

* **MAE**: `r sprintf('%.2f', ins_mae)`
* **Correlation (r)**: `r sprintf('%.3f', ins_r)`

```{r}
#| label: tbl-in-sample
#| tbl-cap: "In-sample metrics by season (2010/11–2023/24)"
in_sample |>
  arrange(Season) |>
  mutate(MAE = as.numeric(sprintf("%.2f", MAE)),
         r   = as.numeric(sprintf("%.3f", r))) |>
  kable(format = "latex", booktabs = TRUE)
```

### Leave-one-season-out (LOSO)

Overall across held-out seasons:

* **Median MAE**: `r sprintf('%.2f', loso_med_mae)`
* **Median r**: `r sprintf('%.3f', loso_med_r)`

```{r}
#| label: tbl-loso
#| tbl-cap: "LOSO per-season holdout performance"
loso |>
  arrange(Season) |>
  mutate(MAE = as.numeric(sprintf("%.2f", MAE)),
         r   = as.numeric(sprintf("%.3f", r))) |>
  kable(format = "latex", booktabs = TRUE)
```

## Prospective 2024/25 PiT

PiT projects the final table using realized points to date plus Pythagorean projections
for remaining fixtures under the fitted coefficients.

```{r}
#| label: tbl-pit-summary
#| tbl-cap: "Bundesliga 2024/25 PiT summary after 9, 18, and 27 rounds."
pit_summary |>
  mutate(MAE = as.numeric(sprintf("%.2f", MAE)),
         r   = as.numeric(sprintf("%.3f", r))) |>
  kable(format = "latex", booktabs = TRUE)
```

### Visual comparisons (predicted vs actual)

```{r}
#| label: fig-r09
#| fig-cap: "Predicted vs actual EoS points using PiT after Round 9 (2024/25)."
knitr::include_graphics(file.path(OUT, "BL_2024_25_PiT_round_09.png"))
```

```{r}
#| label: fig-r18
#| fig-cap: "Predicted vs actual EoS points using PiT after Round 18 (2024/25)."
knitr::include_graphics(file.path(OUT, "BL_2024_25_PiT_round_18.png"))
```

```{r}
#| label: fig-r27
#| fig-cap: "Predicted vs actual EoS points using PiT after Round 27 (2024/25)."
knitr::include_graphics(file.path(OUT, "BL_2024_25_PiT_round_27.png"))
```

### Biggest over/under at Round 27

```{r}
#| label: tbl-top5
#| tbl-cap: "Top-5 absolute deviations between predicted total (after Round 27) and final EoS points, 2024/25."
diff27 |>
  arrange(desc(abs_err)) |>
  slice(1:5) |>
  transmute(
    Team,
    Predicted_Total = round(PredTot, 1),
    Actual_PTS = PTS,
    Error = round(err, 1),
    Abs_Error = round(abs_err, 1),
    Direction = flag
  ) |>
  kable(format = "latex", booktabs = TRUE)
```

# Discussion {#sec-discussion}

## Contributions and Findings
This paper calibrates a four-parameter Pythagorean points model to the Bundesliga, following the formulation discussed in Chapter 5 of the Soccer Analytics text and the approach used by Professor Clive Beggs. We estimate league-specific coefficients on seasons 2010/11–2023/24 and evaluate prospective “points-in-table” (PiT) forecasts for 2024/25 at three checkpoints. The historical fit shows that a simple transformation of goals for and goals against explains end-of-season points with small average error. On the hold-out season, forecasts improve as the campaign progresses: early estimates (Round 9) are noisier, while by Round 27 the predictions are close to the realized totals (see @tbl-pit-summary) and Figures @fig-r09–@fig-r27)). This confirms a practical message from the literature: across a full league schedule, goal production and prevention are strong summary measures of team performance.

## Limitations
The model projects future points using only current goals for/against and matches played. It does not account for opponent strength, injuries, tactical changes, remaining home/away mix, red cards, or schedule congestion. As a result, it will rate a traditionally strong club poorly during an extended slump until the goal balance turns, and it will continue to credit an over-performing side until their goals regress. These omissions are most visible early in a season when fixture lists can be uneven.

## Future Directions
Several extensions would make the model more realistic. First, generate opponent-aware projections for remaining fixtures using xG or rating models and simulate the schedule so home/away and strength of schedule are reflected. Second, allow team form to evolve over time and let (a,b,c,d) drift modestly across seasons via a hierarchical fit. Third, replace pure MAE with a robust loss and report uncertainty—bootstrap or Bayesian intervals—so PiT outputs include ranges, not just points. Finally, in the first 5–10 rounds, shrink expectations toward league averages or pre-season ratings to avoid overreacting to short runs.
